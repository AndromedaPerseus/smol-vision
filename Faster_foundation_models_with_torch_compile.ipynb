{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Faster Foundation Models with `torch.compile`"
      ],
      "metadata": {
        "id": "axYlcDTznci4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Â Introduction to `torch.compile()`"
      ],
      "metadata": {
        "id": "B-yw8KMWsjfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This guide aims to provide a benchmark on the inference speed-ups introduced with `torch.compile()` with no reduction in model performance for foundation models in ðŸ¤— Transformers.\n",
        "\n",
        "Most used `torch.compile` modes are following:\n",
        "\n",
        "- \"default\" is the default mode, which is a good balance between performance and overhead\n",
        "\n",
        "- \"reduce-overhead\" reduces the overhead of python with CUDA graphs, useful for small batches, consumes a lot of memory. As of now only works for CUDA only graphs which do not mutate inputs.\n",
        "\n",
        "If you have a lot of memory to use, the best speed-up is through `reduce-overhead`. How much speed-up one can get depends on the model, so in this tutorial we will check the most used foundation models."
      ],
      "metadata": {
        "id": "AmmT4aDnqgOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OWLv2\n",
        "\n",
        "OWLv2 is a zero-shot object detection model released by Google Brain. We will load base version."
      ],
      "metadata": {
        "id": "5sCfbPTn7wBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the model and processor for OWLv2."
      ],
      "metadata": {
        "id": "joeX3J315K0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)"
      ],
      "metadata": {
        "id": "Ztfcdqkul62z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, Owlv2ForObjectDetection\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
        "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\").to(\"cuda\")\n",
        "\n",
        "texts = [[\"a photo of a bee\", \"a photo of a bird\"]]\n",
        "inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "84npPHCQpHZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now get to benchmarking. We will benchmark the model itself and the compiled model."
      ],
      "metadata": {
        "id": "3AedkjLu5PRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "repetitions = 30\n",
        "timings=np.zeros((repetitions,1))\n",
        "\n",
        "for _ in range(10):\n",
        "    _ = model(**inputs)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for rep in range(repetitions):\n",
        "        torch.cuda.synchronize()\n",
        "        starter.record()\n",
        "        output = model(**inputs)\n",
        "        ender.record()\n",
        "        torch.cuda.synchronize()\n",
        "        curr_time = starter.elapsed_time(ender)\n",
        "        timings[rep] = curr_time\n",
        "\n",
        "mean_syn = np.sum(timings) / repetitions\n",
        "print(mean_syn)\n"
      ],
      "metadata": {
        "id": "RQQSEgkQtXEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
        "timings=np.zeros((repetitions,1))\n",
        "\n",
        "compiled_model = torch.compile(model, mode=\"reduce-overhead\").to(\"cuda\")\n",
        "\n",
        "for _ in range(30):\n",
        "  with torch.no_grad():\n",
        "    _ = compiled_model(**inputs)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for rep in range(repetitions):\n",
        "        torch.cuda.synchronize()\n",
        "        starter.record()\n",
        "        output = compiled_model(**inputs)\n",
        "        ender.record()\n",
        "        torch.cuda.synchronize()\n",
        "        curr_time = starter.elapsed_time(ender)\n",
        "        timings[rep] = curr_time\n",
        "\n",
        "mean_syn = np.sum(timings) / repetitions\n",
        "print(mean_syn)"
      ],
      "metadata": {
        "id": "bEZiNgaupOx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got nearly 40 percent speed-up! You can try bigger models and see how much speed-up you can get."
      ],
      "metadata": {
        "id": "d_0d7DwN6gBt"
      }
    }
  ]
}